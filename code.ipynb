{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "* DataLoad: loads individual raw train/test tables\n",
    "* Preprocessing: all table preprocessing, final merge, returns train/test_dataset\n",
    "* Train: loads train/test_dataset, trains model, train_test_split, data, evaluates using f1-score, returns model\n",
    "* Predict: loads model, train_dataset and test_dataset, compares fills missing columns with zeros, .predict, returns predictions_array\n",
    "* Submit: loads predictions_array, generates submission_df, saves to csv in submissions folder\n",
    "* Pipeline: Wrapper for whole process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODAY = datetime.today().strftime('%Y-%m-%d')\n",
    "TARGET = 'upgrade'\n",
    "ID = 'line_id'\n",
    "ROOT_PATH = 's3://tf-trachack-notebooks/'+'9417-brhuang-unsw'+'/jupyter/jovyan/'\n",
    "DATA_PATH = 's3://tf-trachack-data/212/'\n",
    "SUBMISSION_PATH = ROOT_PATH+f\"submission/{TODAY}.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoad Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoad:\n",
    "    def get_upgrades(training:bool) -> pd.DataFrame:\n",
    "        training_string = \"dev\" if training else \"eval\"\n",
    "        return pd.read_csv(DATA_PATH+f\"data/{training_string}/upgrades.csv\")\n",
    "\n",
    "    def get_customer_info(training:bool) -> pd.DataFrame:\n",
    "        training_string = \"dev\" if training else \"eval\"\n",
    "        return pd.read_csv(DATA_PATH+f\"data/{training_string}/customer_info.csv\")\n",
    "\n",
    "    def get_redemptions(training:bool) -> pd.DataFrame:\n",
    "        training_string = \"dev\" if training else \"eval\"\n",
    "        return pd.read_csv(DATA_PATH+f\"data/{training_string}/redemptions.csv\")\n",
    "    \n",
    "    def get_deactivations(training:bool) -> pd.DataFrame:\n",
    "        training_string = \"dev\" if training else \"eval\"\n",
    "        return pd.read_csv(DATA_PATH+f\"data/{training_string}/deactivations.csv\")\n",
    "    \n",
    "    def get_reactivations(training:bool) -> pd.DataFrame:\n",
    "        training_string = \"dev\" if training else \"eval\"\n",
    "        return pd.read_csv(DATA_PATH+f\"data/{training_string}/reactivations.csv\")\n",
    "    \n",
    "    def get_suspensions(training:bool) -> pd.DataFrame:\n",
    "        training_string = \"dev\" if training else \"eval\"\n",
    "        return pd.read_csv(DATA_PATH+f\"data/{training_string}/suspensions.csv\")\n",
    "    \n",
    "    def get_phone_info(training:bool) -> pd.DataFrame:\n",
    "        training_string = \"dev\" if training else \"eval\"\n",
    "        return pd.read_csv(DATA_PATH+f\"data/{training_string}/phone_info.csv\")\n",
    "\n",
    "    def get_network_usage_domestic(training:bool) -> pd.DataFrame:\n",
    "        training_string = \"dev\" if training else \"eval\"\n",
    "        return pd.read_csv(DATA_PATH+f\"data/{training_string}/network_usage_domestic.csv\")\n",
    "        \n",
    "    def get_lrp_points(training:bool) -> pd.DataFrame:\n",
    "        training_string = \"dev\" if training else \"eval\"\n",
    "        return pd.read_csv(DATA_PATH+f\"data/{training_string}/lrp_points.csv\")\n",
    "\n",
    "    def get_lrp_enrollment(training:bool) -> pd.DataFrame:\n",
    "        training_string = \"dev\" if training else \"eval\"\n",
    "        return pd.read_csv(DATA_PATH+f\"data/{training_string}/lrp_enrollment.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Helper:\n",
    "    def drop_infrequent_categories(df: pd.DataFrame, cols: list, thresh: int = 10000, replace_with: str = \"other\"):\n",
    "            for col in cols:\n",
    "                value_counts = df[col].value_counts()\n",
    "                to_remove = value_counts[value_counts <= thresh].index\n",
    "                df[col].replace(to_remove, replace_with, inplace=True)\n",
    "                \n",
    "    def create_var_sum(df, save=None):\n",
    "        \"\"\"\n",
    "        Input: \n",
    "          df = data frame to summarise\n",
    "          save = string name of dataframe you wish to save\n",
    "        Returns:\n",
    "          A dataframe that breaks down all variables in a dataframe by unique, missing, datatype, and frequency,\n",
    "          Option to save the file\n",
    "        \"\"\"\n",
    "        result = pd.DataFrame({\n",
    "            'Col':          df.columns,\n",
    "            'Unique':       [df[col].nunique() for col in df.columns],\n",
    "            'Missing':      [len(df[col]) - df[col].count() for col in df.columns],\n",
    "            'Datatype':     list(df.dtypes),\n",
    "            'Most Freq':    [list(df[col].value_counts().index[:5]) for col in df.columns],\n",
    "            'Least Freq':   [list(df[col].value_counts().index[-5:]) for col in df.columns]})\n",
    "\n",
    "        if save != None:\n",
    "            result.to_excel(f\"{WRKDIR}/Data/new/'VariableSummary{save}.xls\", index = False)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def get_nonmissing_cols_and_warn(df: pd.DataFrame, cols, verbose=False):\n",
    "        missing_cols = [x for x in cols if x not in df.columns]\n",
    "        if len(missing_cols) > 0:\n",
    "            if verbose:\n",
    "                print(\"The following columns are not in the source data:\\n\", missing_cols)\n",
    "            cols = [x for x in cols if x not in missing_cols]\n",
    "        return cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def preprocess_customer_status(training:bool) -> pd.DataFrame: ## works!\n",
    "        # loading the data\n",
    "        upgrades_df = DataLoad.get_upgrades(training=training)\n",
    "        customer_info_df = DataLoad.get_customer_info(training=training)\n",
    "        redemptions_df = DataLoad.get_redemptions(training=training)\n",
    "        deactivations_df = DataLoad.get_deactivations(training=training)\n",
    "        reactivations_df = DataLoad.get_reactivations(training=training)\n",
    "        suspensions_df = DataLoad.get_suspensions(training=training)\n",
    "        \n",
    "        # cleaning upgrades\n",
    "        if training: \n",
    "            upgrades_df.upgrade.replace(('yes', 'no'), (1, 0), inplace=True)\n",
    "        upgrades_df.drop(['date_observed'],axis=1, inplace=True)\n",
    "        \n",
    "        # cleaning customer_info ## WORKS!\n",
    "        customer_info_df.replace('[NULL]',np.NaN, inplace=True)\n",
    "        customer_info_df.drop(['plan_subtype'], axis = 1, inplace=True)\n",
    "        customer_info_df['plan_name'].fillna(\"not_given\", inplace=True)\n",
    "        customer_info_df.drop(['redemption_date', 'first_activation_date'], axis=1,inplace=True)\n",
    "        ci_cat_features_df = customer_info_df.loc[:,['line_id','carrier', 'plan_name']]\n",
    "        ci_cat_features_df = pd.get_dummies(ci_cat_features_df, columns=['carrier', 'plan_name'])\n",
    "        clean_customer_info_df = pd.merge(customer_info_df, ci_cat_features_df, on='line_id', how='inner')\n",
    "        clean_customer_info_df.drop(['plan_name', 'carrier'], axis = 1, inplace=True)\n",
    "        \n",
    "        # cleaning redemptions ## WORKS!\n",
    "        redemptions_df.drop(['channel','redemption_type','revenue_type', 'gross_revenue'], axis=1, inplace=True)\n",
    "        redemption_count_per_line_id = redemptions_df.groupby(['line_id'])['redemption_date'].count()\n",
    "        redemptions_df = pd.merge(redemptions_df,redemption_count_per_line_id, on='line_id',how='inner')\n",
    "        redemptions_df.drop(['redemption_date_x'], axis=1, inplace=True)\n",
    "        redemptions_df.rename(columns = {'redemption_date_y':'total_redemptions'}, inplace=True)\n",
    "        clean_redemptions_df = redemptions_df.drop_duplicates()\n",
    "        \n",
    "        # cleaning suspensions ## WORKS!\n",
    "        suspensions_cat_df = suspensions_df.groupby('line_id')\n",
    "        suspensions_total_df = suspensions_cat_df.count()\n",
    "        rename_dict = {'suspension_start_date':'total_suspensions',\n",
    "                       'suspension_end_date':'total_unsuspensions',\n",
    "                      }\n",
    "        clean_suspensions_df = suspensions_total_df.rename(columns=rename_dict)\n",
    "        \n",
    "        # clean deactivations ## works !\n",
    "        deactivations_total_df = deactivations_df[['line_id', 'deactivation_date']].groupby('line_id')\n",
    "        deactivations_total_df = deactivations_total_df.count()\n",
    "        clean_deactivations_df = deactivations_total_df.rename(columns={'deactivation_date':'total_deactivations'})\n",
    "        \n",
    "        # clean reactivations ## works !\n",
    "        reactivations_total_df = reactivations_df[['line_id', 'reactivation_date']].groupby('line_id')\n",
    "        reactivations_total_df = reactivations_total_df.count()\n",
    "        reactivations_total_df.rename(columns={'reactivation_date':'total_reactivations'}, inplace=True)\n",
    "        \n",
    "        reactivations_dropping = reactivations_df.copy(deep = True)\n",
    "        Helper.drop_infrequent_categories(reactivations_dropping, ['reactivation_channel'], thresh = 2000)\n",
    "        reactivations_cat_features_df = reactivations_dropping.loc[:,['line_id','reactivation_channel']]\n",
    "        reactivations_cat_features_df = pd.get_dummies(reactivations_cat_features_df, columns=['reactivation_channel'])\n",
    "        reactivations_cat_features_df = reactivations_cat_features_df.groupby('line_id')\n",
    "        reactivations_cat_features_df = reactivations_cat_features_df.sum()\n",
    "        \n",
    "        clean_reactivations_df = pd.merge(reactivations_total_df, reactivations_cat_features_df,on='line_id',how='inner')\n",
    "        \n",
    "        # merging ## works!\n",
    "        final = clean_customer_info_df\n",
    "        \n",
    "        if training: \n",
    "            final = pd.merge(upgrades_df,final,on='line_id',how='inner')\n",
    "        \n",
    "        final = pd.merge(final, clean_redemptions_df,on='line_id',how='left')\n",
    "        final = pd.merge(final, clean_suspensions_df,on='line_id',how='left')\n",
    "        final = pd.merge(final, clean_deactivations_df,on='line_id',how='left')\n",
    "        final = pd.merge(final, clean_reactivations_df,on='line_id',how='left')\n",
    "        final.fillna(0, inplace=True)\n",
    "        return final\n",
    "        \n",
    "    def preprocess_lrp(training:bool) -> pd.DataFrame:\n",
    "        lrp_points = DataLoad.get_lrp_points(training=training)\n",
    "        lrp_enrollment = DataLoad.get_lrp_enrollment(training=training)\n",
    "        upgrades = DataLoad.get_upgrades(training=training)\n",
    "        \n",
    "        if training: \n",
    "            upgrades.upgrade.replace(('yes', 'no'), (1, 0), inplace=True)\n",
    "        \n",
    "        lrpp = lrp_points.copy()\n",
    "        lrp_combine = lrpp.merge(lrp_enrollment, on='line_id', how='outer')\n",
    "        lrp_combine.drop('status', axis=1, inplace=True)\n",
    "        lrp_combine['quantity'].replace({np.nan:0},inplace=True)\n",
    "        lrp_combine['total_quantity'].replace({np.nan:0},inplace=True)\n",
    "        now = datetime.now()\n",
    "        lrp_combine['length_of_membership'] = now - pd.to_datetime(lrp_combine['lrp_enrollment_date'], format='%Y-%m-%d')\n",
    "        lrp_combine['length_of_membership'] = lrp_combine['length_of_membership'].apply(lambda x: x.days)\n",
    "        lrp_combine['update_date'] = lrp_combine.apply(lambda x: x['lrp_enrollment_date'] if pd.isna(x['update_date']) else x['update_date'], axis=1)\n",
    "        lrp_combine['last_interaction'] = (now - pd.to_datetime(lrp_combine['update_date'], format='%Y-%m-%d')).apply(lambda x: x.days)\n",
    "        lrp_combine.drop(['update_date','lrp_enrollment_date'], axis=1, inplace=True)\n",
    "        \n",
    "        # joining with upgrade\n",
    "        up_lrp = upgrades.merge(lrp_combine, on='line_id', how='left')\n",
    "        up_lrp.lrp_enrolled.replace(('Y', np.nan), (1, 0), inplace=True)\n",
    "        up_lrp['total_quantity'] = up_lrp.apply(lambda x: 0 if x['lrp_enrolled'] == 0 else x['total_quantity'], axis=1)\n",
    "        up_lrp['quantity'] = up_lrp.apply(lambda x: 0 if x['lrp_enrolled'] == 0 else x['quantity'], axis=1)\n",
    "        up_lrp['length_of_membership'] = up_lrp.apply(lambda x: 0 if x['lrp_enrolled'] == 0 else x['length_of_membership'], axis=1)\n",
    "        up_lrp['last_interaction'] = up_lrp.apply(lambda x: 0 if x['lrp_enrolled'] == 0 else x['last_interaction'], axis=1)\n",
    "        \n",
    "        # removing dups\n",
    "        up_lrp = up_lrp[up_lrp.groupby('line_id')['length_of_membership'].transform('min') == up_lrp['length_of_membership']]\n",
    "        up_lrp.drop('date_observed', axis=1, inplace=True)\n",
    "        return up_lrp\n",
    "        \n",
    "    def preprocess_phone_info(training:bool) -> pd.DataFrame:\n",
    "        phone_info = DataLoad.get_phone_info(training=training)\n",
    "        upgrades = DataLoad.get_upgrades(training=training)\n",
    "        \n",
    "        # cleaning upgrades\n",
    "        if training: \n",
    "            upgrades.upgrade.replace(('yes', 'no'), (1, 0), inplace=True)\n",
    "        upgrades.drop(['date_observed'],axis=1, inplace=True)\n",
    "        \n",
    "        # cleaning phone_info\n",
    "            # numerical fillnas\n",
    "        num_cols = ['expandable_storage', 'lte', 'lte_advanced', 'lte_category', \n",
    "                    'touch_screen', 'wi_fi', 'year_released']\n",
    "        for col in num_cols:\n",
    "            phone_info[col].fillna(-1.0, inplace=True)\n",
    "        \n",
    "        cat_cols = ['cpu_cores', 'manufacturer', 'os_family', 'os_name', 'os_vendor', 'os_version', \n",
    "                    'sim_size', 'total_ram', 'gsma_device_type', 'gsma_model_name', 'gsma_operating_system', \n",
    "                    'internal_storage_capacity']\n",
    "        for col in cat_cols:\n",
    "            phone_info[col].fillna(\"none\", inplace=True)\n",
    "        \n",
    "        # should also drop gsma_operating_system\n",
    "        drop_cols = ['lte_advanced','wi_fi','touch_screen','gsma_model_name','os_family','os_vendor', \n",
    "                     'year_released', 'lte', 'sim_size']\n",
    "        phone_info.drop(drop_cols, axis =1, inplace = True)\n",
    "        \n",
    "        Helper.drop_infrequent_categories(phone_info, ['lte_category'], thresh = 2000)\n",
    "        Helper.drop_infrequent_categories(phone_info, ['manufacturer'], thresh = 900)\n",
    "        Helper.drop_infrequent_categories(phone_info, ['os_name'], thresh = 10000)\n",
    "        Helper.drop_infrequent_categories(phone_info, ['os_version'], thresh = 1000)\n",
    "        Helper.drop_infrequent_categories(phone_info, ['total_ram'], thresh = 2000)\n",
    "        Helper.drop_infrequent_categories(phone_info, ['cpu_cores'], thresh = 2000)\n",
    "        Helper.drop_infrequent_categories(phone_info, ['gsma_device_type'], thresh = 11000)\n",
    "        phone_info['gsma_operating_system'].replace(['Not Known', 'NONE'], ['none', 'none'], inplace = True)\n",
    "        Helper.drop_infrequent_categories(phone_info, ['gsma_operating_system'], thresh = 12000)\n",
    "        Helper.drop_infrequent_categories(phone_info, ['internal_storage_capacity'], thresh = 6000)\n",
    "        \n",
    "        # one hot encode data\n",
    "#         cat_features = list(phone_info.columns).remove(ID)\n",
    "        pi_cat_features = pd.get_dummies(phone_info, columns=['cpu_cores', 'expandable_storage','gsma_device_type','gsma_operating_system','internal_storage_capacity','lte_category','manufacturer','os_name','os_version','total_ram'])\n",
    "        clean_phone_info = pd.merge(upgrades,pi_cat_features,on='line_id',how='inner')\n",
    "        \n",
    "        return clean_phone_info\n",
    "    \n",
    "    def preprocess_network_usage_domestic(training:bool) -> pd.DataFrame:\n",
    "        network_usage_domestic = DataLoad.get_network_usage_domestic(training=training)\n",
    "        upgrades = DataLoad.get_upgrades(training=training)\n",
    "        last_date = network_usage_domestic[['line_id','date']].groupby(['line_id']).max()\n",
    "        first_date = network_usage_domestic[['line_id','date']].groupby(['line_id']).min()\n",
    "        network_mean = network_usage_domestic.groupby(['line_id']).mean()\n",
    "        network_mean['voice_min_total'] = network_mean.apply(lambda row: row.voice_min_in + row.voice_min_out, axis=1)\n",
    "        network_mean['mms_total'] = network_mean.apply(lambda row: row.mms_in + row.mms_out, axis=1)\n",
    "        network_mean['sms_total'] = network_mean.apply(lambda row: row.sms_in + row.sms_out, axis=1)\n",
    "        network_mean['voice_count_out'] = network_mean.apply(lambda row: row.voice_count_total - row.voice_count_in, axis=1)\n",
    "        network_mean = network_mean.merge(upgrades, left_on='line_id', right_on='line_id')\n",
    "        network_mean = network_mean.merge(first_date, left_on='line_id', right_on='line_id')\n",
    "        network_mean = network_mean.merge(last_date, left_on='line_id', right_on='line_id')\n",
    "        now = datetime.now()\n",
    "        network_mean['length_of_connection'] = now - pd.to_datetime(network_mean['date_x'], format='%Y-%m-%d')\n",
    "        network_mean['length_of_connection'] = network_mean['length_of_connection'].apply(lambda x: x.days)\n",
    "        feature_list = ['line_id','hotspot_kb','total_kb','voice_count_total',\n",
    "                        'voice_min_total','mms_total','sms_total','length_of_connection']\n",
    "        if training:\n",
    "            feature_list.append('upgrade')\n",
    "        \n",
    "        final_network = network_mean[feature_list]\n",
    "        final_network['used_network'] = 1\n",
    "        \n",
    "        total = set(upgrades['line_id'])\n",
    "        net = set(final_network['line_id'])\n",
    "        left_out = list(total.difference(net))\n",
    "        \n",
    "        not_connect = final_network\n",
    "        not_connect = not_connect[0:0]\n",
    "        not_connect['line_id'] = left_out\n",
    "        not_connect.fillna(0, inplace=True)\n",
    "        \n",
    "        if training:\n",
    "            not_connect.drop(['upgrade'], axis=1, inplace = True)\n",
    "        \n",
    "        not_connect = not_connect.merge(upgrades, left_on='line_id', right_on='line_id')\n",
    "        \n",
    "        final_network = pd.concat([final_network, not_connect])\n",
    "        final_network.drop(['date_observed'], axis=1, inplace = True)\n",
    "        \n",
    "        if training:\n",
    "            final_network.upgrade.replace(('yes', 'no'), (1, 0), inplace=True)\n",
    "        \n",
    "        return final_network\n",
    "    \n",
    "    def merge(training:bool,clean_network, clean_customer, clean_lrp, clean_phone_info) -> pd.DataFrame: # nortons merge goes here\n",
    "        # drop upgrade column\n",
    "        if training:\n",
    "            clean_network.drop(['upgrade'], axis=1, inplace=True)\n",
    "            clean_customer.drop(['upgrade'], axis=1, inplace=True)\n",
    "            clean_phone_info.drop(['upgrade'], axis=1, inplace=True)\n",
    "        \n",
    "        df = clean_lrp.merge(clean_customer, on='line_id', how='left')\n",
    "        df = df.merge(clean_phone_info, on='line_id', how='left')\n",
    "        df = df.merge(clean_network, on='line_id', how='left')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def run_training(training_data:pd.DataFrame) -> RandomForestClassifier:\n",
    "        X, y = Train.split_x_y(training_data)\n",
    "        model = Train.train_model(X, y)\n",
    "        return model\n",
    "\n",
    "    def split_x_y(training_data:pd.DataFrame) -> (np.array, np.array):\n",
    "        X = training_data.drop([TARGET, ID], axis=1).values\n",
    "        y = training_data[TARGET].values\n",
    "        return X, y\n",
    "\n",
    "    def train_model(X:np.array, y:np.array) -> RandomForestClassifier:\n",
    "        model = RandomForestClassifier(\n",
    "            bootstrap=False, max_depth=65,max_features='sqrt', \n",
    "            min_samples_leaf=1,min_samples_split=6,n_estimators=1260\n",
    "            )\n",
    "        model.fit(X, y)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predict:\n",
    "    def run_prediction(model:RandomForestClassifier, test_data:pd.DataFrame, training_data:pd.DataFrame) -> (np.array,np.array):\n",
    "        reshaped_test_data = Predict.reshape_test_data(training_data, test_data)\n",
    "        predictions = Predict.get_predictions(model, reshaped_test_data)\n",
    "        return predictions\n",
    "\n",
    "    def reshape_test_data(training_data:pd.DataFrame, test_data:pd.DataFrame) -> pd.DataFrame:\n",
    "        training_data_cols = set(training_data.columns)\n",
    "        test_data_cols = set(test_data.columns)\n",
    "        cols_to_add = list(training_data_cols - test_data_cols)\n",
    "        cols_to_drop = list(test_data_cols - training_data_cols)\n",
    "        \n",
    "        reshaped_test_data = test_data\n",
    "        cols_to_drop = Helper.get_nonmissing_cols_and_warn(test_data, cols_to_drop)\n",
    "        if len(cols_to_drop) > 0:\n",
    "            reshaped_test_data = test_data.drop(cols_to_drop, axis=1)\n",
    "        \n",
    "        padding = pd.DataFrame(0, index=np.arange(len(reshaped_test_data)), columns=list(cols_to_add))\n",
    "        padding.drop('upgrade', axis=1, inplace=True)\n",
    "        \n",
    "        reshaped_test_data = pd.concat([reshaped_test_data, padding], axis=1)\n",
    "        return reshaped_test_data\n",
    "        \n",
    "    def get_predictions(model: RandomForestClassifier, test_data: pd.DataFrame) -> np.array:\n",
    "        X = test_data.drop([ID], axis=1).values\n",
    "        predictions = model.predict(X)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Submit:\n",
    "    def make_submission(ids, predictions: np.array) -> None:\n",
    "        submission_df = Submit.generate_submission_df(ids, predictions)\n",
    "        Submit.save_csv(submission_df)\n",
    "        print('Submitted')\n",
    "\n",
    "    def generate_submission_df(ids, predictions: np.array) -> pd.DataFrame:\n",
    "        submission_dict = {\n",
    "            ID     : ids,\n",
    "            TARGET : predictions\n",
    "        }\n",
    "        submissions_df = pd.DataFrame(submission_dict)\n",
    "        return submissions_df\n",
    "    \n",
    "    def save_csv(submission_df):\n",
    "        submission_df.to_csv(SUBMISSION_PATH,header=True,index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''\n",
    "    This is the main training and testing pipeline for the model note that this draws from the results of \n",
    "    all of the data exploration from other scripts.\n",
    "    '''\n",
    "    train_clean_network = Preprocessing.preprocess_network_usage_domestic(training=True) # works!\n",
    "    train_clean_customer = Preprocessing.preprocess_customer_status(training=True) # works!\n",
    "    train_clean_lrp = Preprocessing.preprocess_lrp(training=True) # works !\n",
    "    train_clean_phone_info = Preprocessing.preprocess_phone_info(training=True) # works !\n",
    "    training_dataset = Preprocessing.merge(True, train_clean_network, train_clean_customer, train_clean_lrp, train_clean_phone_info)\n",
    "\n",
    "    model = Train.run_training(training_dataset)\n",
    "    \n",
    "    test_clean_network = Preprocessing.preprocess_network_usage_domestic(training=False) # works!\n",
    "    test_clean_customer = Preprocessing.preprocess_customer_status(training=False) # works!\n",
    "    test_clean_lrp = Preprocessing.preprocess_lrp(training=False) # works !\n",
    "    test_clean_phone_info = Preprocessing.preprocess_phone_info(training=False) # works !\n",
    "    test_dataset = Preprocessing.merge(False, test_clean_network, test_clean_customer, test_clean_lrp, test_clean_phone_info)\n",
    "\n",
    "    predictions = Predict.run_prediction(model, test_dataset, training_dataset)\n",
    "    Submit.make_submission(test_dataset[ID].values, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:182: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:182: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:4153: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}